---
title: "DDS_CaseStudy2"
author: "Ricco Ferraro"
date: "4/13/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Youtube Presentation here: 
https://youtu.be/DvKO9sucwJw

# Introduction
## Restatment of problem
We wish to analyze the top 3 or more factors which lead to Attrition, Predict Attrition, and Predict Monthly Income based on different attributes of employees. 

## The Data
The dataset is comprised of over 870 employee records, some of whom no longer work at the company. Properties exist like Attrition, MonthlyIncome, Gender, etc. 

# Libraries
```{r echo = FALSE}
# install.packages("aplore3")
# install.packages("randomForest")
# install.packages("rpart")
# install.packages("pheatmap")
# install.packages("plotROC")
# install.packages("MLeval")
# install.packages("vcdExtra")
# install.packages("plotROC")
# install.packages("ResourceSelection")
# install.packages("GGally")
library(aplore3)
library(corrgram)
library(ggplot2)
library(psych)
library(magrittr)
library(caret)
library(tidyr)
library(class)
library(dplyr)
library(plyr)
library(tidyverse)
library(plotly)
library(ggthemes)
library(scales)
library(vcdExtra)
library(pROC)
library(MASS)
library(tidyverse)
library(car)
library(randomForest)
library(pheatmap)
library(glmnet)
library(rpart)
library(car)
library(plotROC)
library(MLeval)
library(ResourceSelection)
library(GGally)
library(tidyverse)
library(magrittr)
library(knitr)
library(rmarkdown)
library(DT)
library(mice)
library(VIM)
library(psych)
library (readr)
library(dataMaid)
library(data.table)
```

```{r}
# Utility function 
isEmpty <- function(column) {
    is.na(column) | column == 0 | column == "" | column == " " | column == "NA" | column == "na" | column == "Na" | column == "nA" | column == "NaN" 
}
```

Import the dataset
Note: we make sure that the levels of Attrition are in the right order
```{r}
CaseStudy2.data <-read.csv("../data/CaseStudy2-data.csv", header = TRUE, sep = ",", stringsAsFactors = TRUE)
summary(CaseStudy2.data)
CaseStudy2.data$Attrition <- ordered(CaseStudy2.data$Attrition, levels=c("Yes", "No"))
str(CaseStudy2.data)
```

# 3. Assess the Data 
## Cleanup Factors for anything that looks categorical in nature. 
```{r}
summary(CaseStudy2.data)
CaseStudy2.data$PerformanceRating <- as.factor(CaseStudy2.data$PerformanceRating)
CaseStudy2.data$RelationshipSatisfaction <- as.factor(CaseStudy2.data$RelationshipSatisfaction)
CaseStudy2.data$StockOptionLevel <- as.factor(CaseStudy2.data$StockOptionLevel)
CaseStudy2.data$TrainingTimesLastYear <- as.factor(CaseStudy2.data$TrainingTimesLastYear)
CaseStudy2.data$EnvironmentSatisfaction <- as.factor(CaseStudy2.data$EnvironmentSatisfaction)
CaseStudy2.data$WorkLifeBalance <- as.factor(CaseStudy2.data$WorkLifeBalance)
CaseStudy2.data$Education <- as.factor(CaseStudy2.data$Education)
CaseStudy2.data$PerformanceRating <- as.factor(CaseStudy2.data$PerformanceRating)
CaseStudy2.data$RelationshipSatisfaction <- as.factor(CaseStudy2.data$RelationshipSatisfaction)
CaseStudy2.data$StockOptionLevel <- as.factor(CaseStudy2.data$StockOptionLevel)
CaseStudy2.data$TrainingTimesLastYear <- as.factor(CaseStudy2.data$TrainingTimesLastYear)
CaseStudy2.data$EnvironmentSatisfaction <- as.factor(CaseStudy2.data$EnvironmentSatisfaction)
CaseStudy2.data$JobInvolvement <- as.factor(CaseStudy2.data$JobInvolvement)
CaseStudy2.data$JobSatisfaction <- as.factor(CaseStudy2.data$JobSatisfaction)
CaseStudy2.data$JobLevel <- as.factor(CaseStudy2.data$JobLevel)

```


## Missing data - There is none
```{r}
# Plot missing data (there should be none)
bdat_mice_clean <- aggr(CaseStudy2.data, col=c('navyblue','yellow'),
                    numbers=TRUE, sortVars=TRUE,
                    labels=names(CaseStudy2.data), cex.axis=.7,
                    gap=3, ylab=c("Missing Data (distribution)","Missing Data (Pattern)"))

```
## DataCleanup
Remove the ID data. Additionally, remove any field which has ONLY 1 value for this exercise. 
```{r}
CaseStudy2.data.NoIdOrUselessData <- CaseStudy2.data %>% dplyr::select(-c("ID", "EmployeeCount", "Over18", "StandardHours"))
```

## Test Splits
Split the data randomly 85% Train data, 15% Test data
```{r}
set.seed(223)
trainIndex <- createDataPartition(CaseStudy2.data.NoIdOrUselessData$Attrition, p = .80, list = FALSE, times = 1)
CaseStudy2.dtrain <- CaseStudy2.data.NoIdOrUselessData[trainIndex,]
CaseStudy2.dtest <- CaseStudy2.data.NoIdOrUselessData[-trainIndex,]
```
Note: We assume that this data does not need to be corrected for serial correlation/autocorrelation. In truth, there may be some serial correlation with employees that were hired later or hired earlier, or just in what year there was more atrition. There are many other possible confounding factors such as the economy that we will ignore for the sake of this analysis. 


Utility Functions for Confusion Matrix With Custom threshold. 
```{r}
confusionMatrixForCustomThreshold <- function(model, data, threshold, probabilities=NULL) {
  if(is.null(probabilities)) {
    probabilities <- predict(model, newdata = data, type = "prob")    
  }
  
  preds2 <- factor(ifelse(probabilities$Yes >= threshold,"Yes", "No"), levels=c("Yes","No"))
  CM.Train <- confusionMatrix(preds2, data$Attrition)
  return(CM.Train)
} 

plotConfusionMatrixByThreshold <- function(model, data, testTitle) {
  thresholdSequence <- seq(0, 1, by = 0.001)
  accuracy<-c()
  sensitivities<-c()
  specificities<-c()
  probabilities <- predict(model, newdata = data, type = "prob")
  for(i in 1:length(thresholdSequence)) {
    confusionMatrix <- confusionMatrixForCustomThreshold(model, data, thresholdSequence[i], probabilities)
    accuracy[i] <- unname(confusionMatrix$overall['Accuracy'])
    sensitivities[i] <- unname(confusionMatrix$byClass['Sensitivity'])
    specificities[i] <- unname(confusionMatrix$byClass['Specificity'])
  }
  
  plot(x=thresholdSequence, y=accuracy,lty=2,lwd=2,col="red", xlab="threshold", ylab="probability", main=testTitle)
  points(x=thresholdSequence, y=sensitivities, col="green")
  points(x=thresholdSequence, y=specificities, col="blue")
  legend("bottomright", legend=c("Accuracy", "Sensitivity", "Specificity"),
         col=c("red", "green", "blue"), lty=1:2, cex=0.8)
}
```

# 3. EDA 
Note: We have an imbalanced dataset. There are FAR more "No" entries than "Yes" entries for attrition. Further we can see that there is multicolinearity between some of the variables. More on this below. 
```{r}
CaseStudy2.data.NoIdOrUselessData %>% ggplot(aes(x=Attrition, fill = Attrition)) + geom_bar() + ggtitle("Attrition Count")
ggpairs(CaseStudy2.data.NoIdOrUselessData,columns=1:6,aes(colour=Attrition), legend =1, progress = FALSE) 
ggpairs(CaseStudy2.data.NoIdOrUselessData,columns=7:11,aes(colour=Attrition), legend =1, progress = FALSE) 
ggpairs(CaseStudy2.data.NoIdOrUselessData,columns=12:17,aes(colour=Attrition), legend =1, progress = FALSE) 
ggpairs(CaseStudy2.data.NoIdOrUselessData,columns=18:22,aes(colour=Attrition), legend =1, progress = FALSE) 
ggpairs(CaseStudy2.data.NoIdOrUselessData,columns=23:32,aes(colour=Attrition), legend =1, progress = FALSE) 

```


## PCA
To see whether or not there is any seperation to begin with (amongs the continuous variables in our dataset, we perform PCA)
```{r}
# PCA will only really work for numerical variables, select only numerical variables. 
# Ignore Employee Count, it is always 1. Standard hours is ALWAYS 80. We can ignore that too. 
reduced.numerical <- CaseStudy2.data.NoIdOrUselessData %>% dplyr::select(Age, DailyRate, DistanceFromHome, EmployeeNumber, HourlyRate, MonthlyIncome, MonthlyRate, NumCompaniesWorked, PercentSalaryHike, TotalWorkingYears, YearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion, YearsWithCurrManager)
pc.result<-prcomp(reduced.numerical, center= TRUE, scale. = TRUE)
summary(pc.result)
pc.result
pc.scores<-data.frame(pc.result$x)
pc.scores$Attrition= as.factor(CaseStudy2.data$Attrition)

plot_ly(x = pc.scores$PC1, y = pc.scores$PC2, z = pc.scores$PC3, type="scatter3d", mode="markers", color=pc.scores$Attrition, title="Principal Components 1-3")
ggpairs(pc.scores,columns=1:5,aes(colour=Attrition), legend =1, , progress = FALSE) 
```
## Cluster and HeatMap
This HeatMap is used to show similarities between data with a non-parameteric hierarchical cluster. Our distance metric is still euclidean, so the clustering of categorical variables may not be totally accurate. 
```{r}
pheatmap::pheatmap(data.matrix(CaseStudy2.data.NoIdOrUselessData), scale = "column")
```
## Multicolinearity
We can see that there are high GVIF values for certain predictors in our model. GVIF is used so that we may include categorical variables as well. There is conflicting information on what to do with GVIF (rules of thumb). We will just keep the high VIF values in mind when building plots to show multicolinarity below. 
``` {r}
CaseStudy2.data.NoIdOrUselessData.Copy <- data.frame(CaseStudy2.data.NoIdOrUselessData)
CaseStudy2.data.NoIdOrUselessData.Copy$AgeDummy <- CaseStudy2.data.NoIdOrUselessData.Copy$Age
data.table(vif(lm(AgeDummy~., data=CaseStudy2.data.NoIdOrUselessData.Copy)))
```

# Factor Analysis: 
Variable Importance
After further analysis, we can see that, when predicting Attrition, that Random Forests seem to perform the best for the models we've tried. Since Random Forest performs the best for our Attrition Classification models, we will use it's ranking of variable importance to guide our comparison of mutivariate (categorical and numerical) variable importance. 
```{r}

ctrl <- trainControl(method="cv", summaryFunction=twoClassSummary, classProbs=T, savePredictions = T, number = 5)
fit2.HigherComplexity.Rf <- train(Attrition ~ ., data = CaseStudy2.data.NoIdOrUselessData, method = 'rf', trControl = ctrl, ntree = 200, metric = "ROC")

fit2.HigherComplexity.Rf$metric
fit2.HigherComplexity.Rf

# Train Confusion Matrix
plotConfusionMatrixByThreshold(fit2.HigherComplexity.Rf, CaseStudy2.dtrain, "Train Confusion Matrix Stats - Random Forest")
threshold <- .4
print("Train Confusion Matrix")
print(paste("Threshold was:", threshold))
confusionMatrixForCustomThreshold(fit2.HigherComplexity.Rf, CaseStudy2.dtrain, threshold)

# Test Confusion Matrix
plotConfusionMatrixByThreshold(fit2.HigherComplexity.Rf, CaseStudy2.dtest, "Test Confusion Matrix Stats - Random Forest")
print("Test Confusion Matrix")
print(paste("Threshold was:", threshold))
confusionMatrixForCustomThreshold(fit2.HigherComplexity.Rf, CaseStudy2.dtest, threshold)

summary(fit2.HigherComplexity.Rf$finalModel)

# ROC Plot
res <- evalm(fit2.HigherComplexity.Rf,gnames=c('Train'), title='ROC: RandomForest') 
plot(varImp(fit2.HigherComplexity.Rf, scale = FALSE), top = 8, main="Variable Importance: Random Forest")
```

## Variable Importance MutliColinarity Analysis and Further EDA
Here we plot many of the "top" variables against one another to show multicolinearity between some of the different variables. Notice that 'MonthlyIncome' is highly colinear with 'TotalWorkingYears'. It is also highly multicolinear with 'YearsAtCompany'. As such, we can pick MonthlyIncome and leave out TotalWorkingYears as well as YearsAtCompany from the "top factors" list. 
```{r}
Top10VariableData <- CaseStudy2.data.NoIdOrUselessData %>% dplyr::select(Attrition, MonthlyIncome, OverTime, Age, JobRole, DailyRate, TotalWorkingYears, EmployeeNumber, MonthlyRate, DistanceFromHome)
ggpairs(Top10VariableData,aes(colour=Attrition), columns=2:9, legend =1, progress = FALSE) 

CaseStudy2.data.NoIdOrUselessData %>% ggplot(aes(x=Age, y=MonthlyIncome, colour = Attrition)) + geom_point() + geom_smooth() + ggtitle("MonthlyIncome vs. Age")
CaseStudy2.data.NoIdOrUselessData %>% ggplot(aes(x=TotalWorkingYears, y=MonthlyIncome, colour = Attrition)) + geom_point() + geom_smooth() + ggtitle("MonthlyIncome vs. TotalWorkingYears")
CaseStudy2.data.NoIdOrUselessData %>% ggplot(aes(x=YearsAtCompany, y=MonthlyIncome, colour = Attrition)) + geom_point() + geom_smooth() + ggtitle("MonthlyIncome vs. YearsAtCompany")
CaseStudy2.data.NoIdOrUselessData %>% ggplot(aes(x=DailyRate, y=MonthlyIncome, colour = Attrition)) + geom_point() + geom_smooth() + ggtitle("MonthlyIncome vs. DailyRate")
CaseStudy2.data.NoIdOrUselessData %>% ggplot(aes(x=MonthlyRate, y=MonthlyIncome, colour = Attrition)) + geom_point() + geom_smooth() + ggtitle("MonthlyIncome vs. MonthlyRate")
CaseStudy2.data.NoIdOrUselessData %>% ggplot(aes(x=HourlyRate, y=MonthlyRate, colour = Attrition)) + geom_point() + geom_smooth() + ggtitle("MonthlyRate vs. HourlyRate")

```

# Factor Analysis: 
Variable Importance, Multi-colinearity corrected. Now that we know which continuous variables we can remove due to multi-colinearity, let's re-fit the random forest and look at variable importance a second time. 
```{r}

ctrl <- trainControl(method="cv", summaryFunction=twoClassSummary, classProbs=T, savePredictions = T, number = 5)
fit2.HigherComplexity.Rf <- train(Attrition ~ . -I(TotalWorkingYears) -I(YearsAtCompany), data = CaseStudy2.data.NoIdOrUselessData, method = 'rf', trControl = ctrl, ntree = 200, metric = "ROC")

fit2.HigherComplexity.Rf$metric

# Train Confusion Matrix
plotConfusionMatrixByThreshold(fit2.HigherComplexity.Rf, CaseStudy2.dtrain, "Train Confusion Matrix Stats - Random Forest")
threshold <- .4
print("Train Confusion Matrix")
print(paste("Threshold was:", threshold))
confusionMatrixForCustomThreshold(fit2.HigherComplexity.Rf, CaseStudy2.dtrain, threshold)

# Test Confusion Matrix
plotConfusionMatrixByThreshold(fit2.HigherComplexity.Rf, CaseStudy2.dtest, "Test Confusion Matrix Stats - Random Forest")
print("Test Confusion Matrix")
print(paste("Threshold was:", threshold))
confusionMatrixForCustomThreshold(fit2.HigherComplexity.Rf, CaseStudy2.dtest, threshold)

summary(fit2.HigherComplexity.Rf$finalModel)

# ROC Plot
res <- evalm(fit2.HigherComplexity.Rf,gnames=c('Train'), title='ROC: RandomForest') 
plot(varImp(fit2.HigherComplexity.Rf, scale = FALSE), top = 5, main="Variable Importance: Random Forest")
```


# Classification
## Random Forest
We now classify with Random Forest with ALl variables. Since we only care about prediction and not interpretation, we can just use all the multicolinear variables and not worry about it. Note: we tune the model to use a threshold of 0.38 to account for the imbalance in our dataset.
```{r}
ctrl <- trainControl(method="cv", summaryFunction=twoClassSummary, classProbs=T, savePredictions = T, number = 10)
fit2.HigherComplexity.Rf <- train(Attrition ~ ., data = CaseStudy2.dtrain, method = 'rf', trControl = ctrl)

# Train Confusion Matrix
plotConfusionMatrixByThreshold(fit2.HigherComplexity.Rf, CaseStudy2.dtrain, "Train Confusion Matrix Stats - Random Forest")
threshold <- .145
print("Train Confusion Matrix")
print(paste("Threshold was:", threshold))
confusionMatrixForCustomThreshold(fit2.HigherComplexity.Rf, CaseStudy2.dtrain, threshold)

# Test Confusion Matrix
plotConfusionMatrixByThreshold(fit2.HigherComplexity.Rf, CaseStudy2.dtest, "Test Confusion Matrix Stats - Random Forest")
print("Test Confusion Matrix")
print(paste("Threshold was:", threshold))
confusionMatrixForCustomThreshold(fit2.HigherComplexity.Rf, CaseStudy2.dtest, threshold)

summary(fit2.HigherComplexity.Rf$finalModel)

# ROC Plot
res <- evalm(fit2.HigherComplexity.Rf,gnames=c('Train'), title='ROC: RandomForest') 

pred2 <- predict(fit2.HigherComplexity.Rf, newdata = CaseStudy2.dtest, type="prob")
test1 <- evalm(data.frame(pred2, CaseStudy2.dtest$Attrition),  title='Test ROC: Logistic Regression-LASSO')
```

# Logistic Regression with LASSO for variable selection (AND prediction)
Note it appears that Logistic Regression with LASSO L1-regularization is absolutely outperformed by RandomForests. 
```{r}
## Logistic Regression: Fit Model - 
ctrl <- trainControl(method="cv", summaryFunction=twoClassSummary, classProbs=T, savePredictions = T, number = 5, search="grid")
fit2.HigherComplexity.Lasso <- train(Attrition ~ (.)^2 + I(Age^2) + I(DailyRate^2)+ I(DistanceFromHome^2)+ I(EmployeeNumber^2)+ I(HourlyRate^2)+ I(MonthlyIncome^2)+ I(MonthlyRate^2)+ I(NumCompaniesWorked^2)+ I(PercentSalaryHike^2) + I(TotalWorkingYears^2) + I(YearsAtCompany^2), data = CaseStudy2.dtrain, method = 'glmnet', trControl = ctrl, metric = "ROC", tuneGrid = data.frame(alpha = 1, lambda = 10^seq(-2, -1.75, by = 0.0001)))

summary(fit2.HigherComplexity.Lasso)
coefficients <- coef(fit2.HigherComplexity.Lasso$finalModel, fit2.HigherComplexity.Lasso$bestTune$lambda)
# coefficients@Dimnames[[1]][which(coefficients > .0001 |coefficients < -.0001 )]

# Train Confusion Matrix
plotConfusionMatrixByThreshold(fit2.HigherComplexity.Lasso, CaseStudy2.dtrain, "Train Confusion Matrix Stats - Lasso")
threshold <- .5
print("Train Confusion Matrix")
print(paste("Threshold was:", threshold))
confusionMatrixForCustomThreshold(fit2.HigherComplexity.Lasso, CaseStudy2.dtrain, threshold)

# Test Confusion Matrix
plotConfusionMatrixByThreshold(fit2.HigherComplexity.Lasso, CaseStudy2.dtest, "Test Confusion Matrix Stats - Lasso")
print("Test Confusion Matrix")
print(paste("Threshold was:", threshold))
confusionMatrixForCustomThreshold(fit2.HigherComplexity.Lasso, CaseStudy2.dtest, threshold)

# ROC Plot
res <- evalm(fit2.HigherComplexity.Lasso,gnames=c('Train'), title='ROC: Logistic Regression-LASSO') 
#Train 
res <- evalm(fit2.HigherComplexity.Lasso,gnames=c('Train'), title='Train ROC: Logistic Regression-LASSO') 

#Test 
pred2 <- predict(fit2.HigherComplexity.Lasso, newdata = CaseStudy2.dtest, type="prob")
test1 <- evalm(data.frame(pred2, CaseStudy2.dtest$Attrition),  title='Test ROC: Logistic Regression-LASSO')
summary(fit2.HigherComplexity.Lasso)
```


# Regression: 
## GLM with Lasso. For prediction of Monthly Income use Multiple Linear Regression with Lasso L1-Regularization. Note that the RSME is well below the requirement (~$500). Although we can see some overfitting on the training set, the test prediction is still acceptably low. 
```{r}
ctrl <- trainControl(method="cv", number = 5, search="grid")
CaseStudy2.dtrain.lassoReg <- data.frame(CaseStudy2.dtrain)
CaseStudy2.dtrain.lassoReg$MonthlyIncome <- 0
fit2.HigherComplexity.Lasso.Regression <- train(MonthlyIncome ~ (.)^2 + I(Age^2) + I(DailyRate^2)+ I(DistanceFromHome^2)+ I(EmployeeNumber^2)+ I(HourlyRate^2)+ I(MonthlyIncome^2)+ I(MonthlyRate^2)+ I(NumCompaniesWorked^2)+ I(PercentSalaryHike^2) + I(TotalWorkingYears^2) + I(YearsAtCompany^2), data = CaseStudy2.dtrain, method = 'glmnet', trControl = ctrl, tuneGrid = data.frame(alpha =  1, lambda = 10^seq(-2, -1.75, by = 0.0001)))

summary(fit2.HigherComplexity.Lasso.Regression)
coefficients <- coef(fit2.HigherComplexity.Lasso.Regression$finalModel, fit2.HigherComplexity.Lasso.Regression$bestTune$lambda)

train_prediction=predict(fit2.HigherComplexity.Lasso.Regression, CaseStudy2.dtrain)
train_ASE<-mean((CaseStudy2.dtrain$MonthlyIncome-train_prediction)^2)
print("Train RSME")
(train_ASE)^0.5

test_prediction=predict(fit2.HigherComplexity.Lasso.Regression, CaseStudy2.dtest)
test_ASE<-mean((CaseStudy2.dtest$MonthlyIncome-test_prediction)^2)
print("Test RSME")
(test_ASE)^0.5

print("Use this method! RSME is acceptable!")

```

## KNN
Note Caret selects (via Cross-validation) k=5 as the optimal k-value (using RSME as the metric and cross-validation). Unfortunately, BOTH the Train and Test RSME are BAD! This is likely because we've only used numerical variables in our model (since they are the only ones that work for KNN)
```{r}
reduced.numerical.train <- CaseStudy2.dtrain %>% dplyr::select(Age, DailyRate, DistanceFromHome, EmployeeNumber, HourlyRate, MonthlyIncome, MonthlyRate, NumCompaniesWorked, PercentSalaryHike, TotalWorkingYears, YearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion, YearsWithCurrManager)

reduced.numerical.test <- CaseStudy2.dtest %>% dplyr::select(Age, DailyRate, DistanceFromHome, EmployeeNumber, HourlyRate, MonthlyIncome, MonthlyRate, NumCompaniesWorked, PercentSalaryHike, TotalWorkingYears, YearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion, YearsWithCurrManager)

ctrl <- trainControl(method="repeatedcv",repeats = 5) 
knnFit <- train(MonthlyIncome ~ ., data = reduced.numerical.train, method = "knn", trControl = ctrl, preProcess = c("center","scale"), tuneLength = 20)

train_prediction=predict(knnFit, reduced.numerical.train)
train_ASE<-mean((reduced.numerical.train$MonthlyIncome-train_prediction)^2)
print("Train RSME")
(train_ASE)^0.5

test_prediction=predict(knnFit, reduced.numerical.test)
test_ASE<-mean((reduced.numerical.test$MonthlyIncome-test_prediction)^2)
print("Test RSME")
(test_ASE)^0.5

print("RSME is BAD for KNN!!! DON'T USE Regression")
```
Predicting Competition set
```{r}
CaseStudy2CompSet.No.Salary$MonthlyIncome <- 0
CaseStudy2CompSet.No.Salary$PerformanceRating <- as.factor(CaseStudy2CompSet.No.Salary$PerformanceRating)
CaseStudy2CompSet.No.Salary$RelationshipSatisfaction <- as.factor(CaseStudy2CompSet.No.Salary$RelationshipSatisfaction)
CaseStudy2CompSet.No.Salary$StockOptionLevel <- as.factor(CaseStudy2CompSet.No.Salary$StockOptionLevel)
CaseStudy2CompSet.No.Salary$TrainingTimesLastYear <- as.factor(CaseStudy2CompSet.No.Salary$TrainingTimesLastYear)
CaseStudy2CompSet.No.Salary$EnvironmentSatisfaction <- as.factor(CaseStudy2CompSet.No.Salary$EnvironmentSatisfaction)
CaseStudy2CompSet.No.Salary$WorkLifeBalance <- as.factor(CaseStudy2CompSet.No.Salary$WorkLifeBalance)
CaseStudy2CompSet.No.Salary$Education <- as.factor(CaseStudy2CompSet.No.Salary$Education)
CaseStudy2CompSet.No.Salary$PerformanceRating <- as.factor(CaseStudy2CompSet.No.Salary$PerformanceRating)
CaseStudy2CompSet.No.Salary$RelationshipSatisfaction <- as.factor(CaseStudy2CompSet.No.Salary$RelationshipSatisfaction)
CaseStudy2CompSet.No.Salary$StockOptionLevel <- as.factor(CaseStudy2CompSet.No.Salary$StockOptionLevel)
CaseStudy2CompSet.No.Salary$TrainingTimesLastYear <- as.factor(CaseStudy2CompSet.No.Salary$TrainingTimesLastYear)
CaseStudy2CompSet.No.Salary$EnvironmentSatisfaction <- as.factor(CaseStudy2CompSet.No.Salary$EnvironmentSatisfaction)
CaseStudy2CompSet.No.Salary$JobInvolvement <- as.factor(CaseStudy2CompSet.No.Salary$JobInvolvement)
CaseStudy2CompSet.No.Salary$JobSatisfaction <- as.factor(CaseStudy2CompSet.No.Salary$JobSatisfaction)
CaseStudy2CompSet.No.Salary$JobLevel <- as.factor(CaseStudy2CompSet.No.Salary$JobLevel)
competition_prediction <- predict(fit2.HigherComplexity.Lasso.Regression, CaseStudy2CompSet.No.Salary)
output.salary <- data.frame( ID=CaseStudy2CompSet.No.Salary$ID, Attrition=competition_prediction)
write.csv(output.salary, "Case2PredictionsFerraroSalary.csv")

```

```{r}
CaseStudy2CompSet.No.Attrition$MonthlyIncome <- 0
CaseStudy2CompSet.No.Attrition$PerformanceRating <- as.factor(CaseStudy2CompSet.No.Attrition$PerformanceRating)
CaseStudy2CompSet.No.Attrition$RelationshipSatisfaction <- as.factor(CaseStudy2CompSet.No.Attrition$RelationshipSatisfaction)
CaseStudy2CompSet.No.Attrition$StockOptionLevel <- as.factor(CaseStudy2CompSet.No.Attrition$StockOptionLevel)
CaseStudy2CompSet.No.Attrition$TrainingTimesLastYear <- as.factor(CaseStudy2CompSet.No.Attrition$TrainingTimesLastYear)
CaseStudy2CompSet.No.Attrition$EnvironmentSatisfaction <- as.factor(CaseStudy2CompSet.No.Attrition$EnvironmentSatisfaction)
CaseStudy2CompSet.No.Attrition$WorkLifeBalance <- as.factor(CaseStudy2CompSet.No.Attrition$WorkLifeBalance)
CaseStudy2CompSet.No.Attrition$Education <- as.factor(CaseStudy2CompSet.No.Attrition$Education)
CaseStudy2CompSet.No.Attrition$PerformanceRating <- as.factor(CaseStudy2CompSet.No.Attrition$PerformanceRating)
CaseStudy2CompSet.No.Attrition$RelationshipSatisfaction <- as.factor(CaseStudy2CompSet.No.Attrition$RelationshipSatisfaction)
CaseStudy2CompSet.No.Attrition$StockOptionLevel <- as.factor(CaseStudy2CompSet.No.Attrition$StockOptionLevel)
CaseStudy2CompSet.No.Attrition$TrainingTimesLastYear <- as.factor(CaseStudy2CompSet.No.Attrition$TrainingTimesLastYear)
CaseStudy2CompSet.No.Attrition$EnvironmentSatisfaction <- as.factor(CaseStudy2CompSet.No.Attrition$EnvironmentSatisfaction)
CaseStudy2CompSet.No.Attrition$JobInvolvement <- as.factor(CaseStudy2CompSet.No.Attrition$JobInvolvement)
CaseStudy2CompSet.No.Attrition$JobSatisfaction <- as.factor(CaseStudy2CompSet.No.Attrition$JobSatisfaction)
CaseStudy2CompSet.No.Attrition$JobLevel <- as.factor(CaseStudy2CompSet.No.Attrition$JobLevel)
competition_prediction.prob <- predict(fit2.HigherComplexity.Rf, CaseStudy2CompSet.No.Attrition, type="prob")
competition_prediction <- factor(ifelse(competition_prediction.prob$Yes >= threshold,"Yes", "No"), levels=c("Yes","No"))
output.salary <- data.frame( ID=CaseStudy2CompSet.No.Attrition$ID, Attrition=competition_prediction)
output.salary
write.csv(output.salary, "Case2PredictionsFerraroAttrition.csv")
```
